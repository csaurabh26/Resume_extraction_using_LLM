{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJnkRnz6oFjI",
        "outputId": "6d985007-bcb8-4f19-fcf8-9b5af100c27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "!pip install evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i2uV-NAEoLMb",
        "outputId": "ea1dceb5-23ce-4bde-f61b-958ccb2a8e3b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import (\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    GPT2Tokenizer, GPT2Config, GPT2ForSequenceClassification,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n"
      ],
      "metadata": {
        "id": "dh3VFoJcoT4w"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "file_path = '/content/drive/MyDrive/Resume (1).csv'  # Update this path as needed\n",
        "resume_data = pd.read_csv(file_path)\n",
        "\n",
        "# Combine relevant columns into a single text field\n",
        "resume_data['Resume'] = (\n",
        "    resume_data['Name'].astype(str) + \" \" +\n",
        "    resume_data['Email'].astype(str) + \" \" +\n",
        "    resume_data['Phone'].astype(str) + \" \" +\n",
        "    resume_data['Skills'].astype(str) + \" \" +\n",
        "    resume_data['Experience'].astype(str) + \" \" +\n",
        "    resume_data['Education'].astype(str) + \" \" +\n",
        "    resume_data['Designation'].astype(str)\n",
        ")\n",
        "\n",
        "# Limit the dataset size for efficiency\n",
        "sample_size = min(500, len(resume_data))\n",
        "resume_data = resume_data.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Add labels (example: use 'Designation' as target)\n",
        "resume_data['Label'] = resume_data['Designation'].astype('category').cat.codes\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X = resume_data['Resume']\n",
        "y = resume_data['Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "KgltCYtSoWx3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResumeDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "GPIINDQTogev"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and datasets for RoBERTa\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "max_length = 128\n",
        "\n",
        "roberta_train_dataset = ResumeDataset(X_train.tolist(), y_train.tolist(), roberta_tokenizer, max_length)\n",
        "roberta_test_dataset = ResumeDataset(X_test.tolist(), y_test.tolist(), roberta_tokenizer, max_length)\n",
        "\n",
        "# Load RoBERTa model\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(y.unique()))\n",
        "\n",
        "# Training arguments for RoBERTa\n",
        "roberta_training_args = TrainingArguments(\n",
        "    output_dir='./roberta_results',\n",
        "    eval_strategy='epoch',\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./roberta_logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Trainer for RoBERTa\n",
        "roberta_trainer = Trainer(\n",
        "    model=roberta_model,\n",
        "    args=roberta_training_args,\n",
        "    train_dataset=roberta_train_dataset,\n",
        "    eval_dataset=roberta_test_dataset\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2xdFLqdoisK",
        "outputId": "54b9821a-2033-4075-9fc6-d6115ae799d2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training RoBERTa...\")\n",
        "roberta_trainer.train()\n",
        "\n",
        "# Evaluate RoBERTa\n",
        "roberta_predictions, roberta_labels, _ = roberta_trainer.predict(roberta_test_dataset)\n",
        "roberta_logits = roberta_predictions\n",
        "roberta_predicted_classes = np.argmax(roberta_logits, axis=-1)\n",
        "roberta_accuracy = np.mean(roberta_predicted_classes == roberta_labels)\n",
        "\n",
        "print(\"RoBERTa Evaluation Results:\")\n",
        "print(f\"Accuracy: {roberta_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "uTxLIqcnolYY",
        "outputId": "a46e2038-3e34-4f89-b1ca-45632907999e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RoBERTa...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 03:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.536800</td>\n",
              "      <td>1.312519</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Evaluation Results:\n",
            "Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and datasets for GPT-2\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token  # Use EOS token as padding\n",
        "\n",
        "gpt_train_dataset = ResumeDataset(X_train.tolist(), y_train.tolist(), gpt_tokenizer, max_length)\n",
        "gpt_test_dataset = ResumeDataset(X_test.tolist(), y_test.tolist(), gpt_tokenizer, max_length)\n",
        "\n",
        "# Configure GPT-2 with padding token\n",
        "# Configure GPT-2 with padding token and set number of labels\n",
        "gpt_config = GPT2Config.from_pretrained('gpt2')\n",
        "gpt_config.pad_token_id = gpt_tokenizer.pad_token_id  # Set padding token\n",
        "gpt_config.num_labels = len(y.unique())  # Set the number of labels for classification\n",
        "\n",
        "# Initialize GPT-2 for sequence classification\n",
        "gpt_model = GPT2ForSequenceClassification.from_pretrained('gpt2', config=gpt_config)\n",
        "\n",
        "# Training arguments for GPT-2\n",
        "gpt_training_args = TrainingArguments(\n",
        "    output_dir='./gpt_results',\n",
        "    eval_strategy='epoch',\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=1,  # Use batch size of 1 to avoid padding issues\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./gpt_logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Trainer for GPT-2\n",
        "gpt_trainer = Trainer(\n",
        "    model=gpt_model,\n",
        "    args=gpt_training_args,\n",
        "    train_dataset=gpt_train_dataset,\n",
        "    eval_dataset=gpt_test_dataset,\n",
        "    tokenizer=gpt_tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilUptDHsopO9",
        "outputId": "797a421b-949d-484d-aa4b-ec11a4cb39cb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-31-c9733fb1929a>:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  gpt_trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training GPT...\")\n",
        "gpt_trainer.train()\n",
        "\n",
        "# Evaluate GPT-2\n",
        "gpt_predictions, gpt_labels, _ = gpt_trainer.predict(gpt_test_dataset)\n",
        "gpt_logits = gpt_predictions\n",
        "gpt_predicted_classes = np.argmax(gpt_logits, axis=-1)\n",
        "gpt_accuracy = np.mean(gpt_predicted_classes == gpt_labels)\n",
        "\n",
        "print(\"GPT Evaluation Results:\")\n",
        "print(f\"Accuracy: {gpt_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "jSPzWgWjqFlf",
        "outputId": "7b0a03d1-5788-469b-c288-24f78482e135"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GPT...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [80/80 05:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>0.002924</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT Evaluation Results:\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary of Results:\")\n",
        "print(f\"RoBERTa Accuracy: {roberta_accuracy}\")\n",
        "print(f\"GPT Accuracy: {gpt_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9g6N_gorTAO",
        "outputId": "7908f96e-b2e0-4d83-d73c-394c00d0be0a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of Results:\n",
            "RoBERTa Accuracy: 0.8\n",
            "GPT Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning RoBERTa\n",
        "roberta_finetune_args = TrainingArguments(\n",
        "    output_dir='./roberta_finetune_results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,  # Reduced learning rate for fine-tuning\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,  # More epochs for fine-tuning\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./roberta_finetune_logs',\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,  # Save only the last 2 checkpoints\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "roberta_finetune_trainer = Trainer(\n",
        "    model=roberta_model,\n",
        "    args=roberta_finetune_args,\n",
        "    train_dataset=roberta_train_dataset,\n",
        "    eval_dataset=roberta_test_dataset,\n",
        "    tokenizer=roberta_tokenizer\n",
        ")\n",
        "\n",
        "print(\"Fine-tuning RoBERTa...\")\n",
        "roberta_finetune_trainer.train()\n",
        "\n",
        "# Evaluate fine-tuned RoBERTa\n",
        "print(\"Evaluating fine-tuned RoBERTa...\")\n",
        "roberta_predictions, roberta_labels, _ = roberta_finetune_trainer.predict(roberta_test_dataset)\n",
        "roberta_logits = roberta_predictions\n",
        "roberta_predicted_classes = np.argmax(roberta_logits, axis=-1)\n",
        "roberta_accuracy = np.mean(roberta_predicted_classes == roberta_labels)\n",
        "\n",
        "print(\"Fine-tuned RoBERTa Results:\")\n",
        "print(f\"Accuracy: {roberta_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "0I2pVVC8vrZF",
        "outputId": "a66437dd-bb0e-4d61-a655-ba28627a790c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-34-59a56b1b3fa0>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  roberta_finetune_trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 11:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.264498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.081997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.467200</td>\n",
              "      <td>0.052213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating fine-tuned RoBERTa...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned RoBERTa Results:\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning GPT-2\n",
        "gpt_finetune_args = TrainingArguments(\n",
        "    output_dir='./gpt_finetune_results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,  # Reduced learning rate for fine-tuning\n",
        "    per_device_train_batch_size=2,  # Slightly increased batch size\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,  # More epochs for fine-tuning\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./gpt_finetune_logs',\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,  # Save only the last 2 checkpoints\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "gpt_finetune_trainer = Trainer(\n",
        "    model=gpt_model,\n",
        "    args=gpt_finetune_args,\n",
        "    train_dataset=gpt_train_dataset,\n",
        "    eval_dataset=gpt_test_dataset,\n",
        "    tokenizer=gpt_tokenizer\n",
        ")\n",
        "\n",
        "print(\"Fine-tuning GPT...\")\n",
        "gpt_finetune_trainer.train()\n",
        "\n",
        "# Evaluate fine-tuned GPT\n",
        "print(\"Evaluating fine-tuned GPT...\")\n",
        "gpt_predictions, gpt_labels, _ = gpt_finetune_trainer.predict(gpt_test_dataset)\n",
        "gpt_logits = gpt_predictions\n",
        "gpt_predicted_classes = np.argmax(gpt_logits, axis=-1)\n",
        "gpt_accuracy = np.mean(gpt_predicted_classes == gpt_labels)\n",
        "\n",
        "print(\"Fine-tuned GPT Results:\")\n",
        "print(f\"Accuracy: {gpt_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "oyyxxtezsUMy",
        "outputId": "6070c192-4c3c-4f6a-bd4b-daa4a3d31e05"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning GPT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-35-90338435fe05>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  gpt_finetune_trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 13:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.000017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating fine-tuned GPT...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned GPT Results:\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}